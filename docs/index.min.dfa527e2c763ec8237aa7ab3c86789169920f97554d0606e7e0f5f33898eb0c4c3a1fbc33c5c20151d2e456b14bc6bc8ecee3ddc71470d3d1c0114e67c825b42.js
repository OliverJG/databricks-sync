var suggestions=document.getElementById('suggestions'),userinput=document.getElementById('userinput');document.addEventListener('keydown',inputFocus);function inputFocus(a){a.keyCode===191&&(a.preventDefault(),userinput.focus()),a.keyCode===27&&(userinput.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement);let c=0;b.keyCode===38?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var c=new FlexSearch({profile:'score',tokenize:"full",encode:"advanced",cache:!0,doc:{id:'id',field:['title','description','content'],store:['href','title','description']}}),b=0,e=[{id:b+0,href:"/docs/getting-started/introduction/",title:"Introduction",description:"A tool used to synchronize a source Databricks deployment with a target Databricks deployment. Useful for migration and Disaster Recovery.",content:'\u003ch2 id="reference-architecture"\u003eReference Architecture\u003c/h2\u003e\n\u003cfigure class="border-0"\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_20x0_resize_box_2.png" data-srcset="https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_900x0_resize_box_2.png 900w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_800x0_resize_box_2.png 800w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_700x0_resize_box_2.png 700w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_600x0_resize_box_2.png 600w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_500x0_resize_box_2.png 500w" width="2208" height="1048" alt="SolutionArchitecture"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_900x0_resize_box_2.png 900w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_800x0_resize_box_2.png 800w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_700x0_resize_box_2.png 700w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_600x0_resize_box_2.png 600w,https://databrickslabs.github.io/databricks-sync/docs/getting-started/introduction/solution-arch_hudacf06c995c8c4a396662145c967f4e3_374691_500x0_resize_box_2.png 500w" src="/docs/getting-started/introduction/solution-arch.png" width="2208" height="1048" alt="SolutionArchitecture"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003e\u003cem\u003eDatabricks Sync Reference Architecture\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="quickstart"\u003eQuickstart\u003c/h2\u003e\n\u003ch3 id="basic-setup"\u003eBasic Setup\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eConfigure \u003ca href="https://github.com/databrickslabs/databricks-sync/blob/master/tests/integration_test.yaml"\u003eYAML file\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eExport object permissions and import them to the target with the object\u003c/li\u003e\n\u003cli\u003eTODO: Add examples for different scenarios:\n\u003cul\u003e\n\u003cli\u003eBackup and Restore\u003c/li\u003e\n\u003cli\u003eCI/CD\u003c/li\u003e\n\u003cli\u003eDisaster Recovery Sync\u003c/li\u003e\n\u003cli\u003eBatch modification (will require Terraform Object Import support)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="common-commands"\u003eCommon commands\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ databricks-sync init my-export-config\n\n$ databricks-sync  export \\\n    --profile \u0026lt;db cli profile\u0026gt; \\\n    --git-ssh-url git@github.com:..../.....git \\\n    -c ....test.yaml\n\noptional flags:\n    -v DEBUG\n    --dry-run\n    --dask\n    --branch # support new main name convention\n\n$ GIT_PYTHON_TRACE=full databricks-sync import \\\n    -g git@github.com:.../....git \\\n    --profile dr_tagert \\\n    --databricks-object-type cluster_policy \\\n    --artifact-dir ..../dir \\\n    --plan \\\n    --skip-refresh \\\n    --revision ....\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eControl the databricks provider version by using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport DATABRICKS_TERRAFORM_PROVIDER_VERSION=\u0026quot;\u0026lt;version here\u0026gt;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="backend-instructions-storing-terraform-state-in-azure-blob-or-aws-s3"\u003eBackend Instructions (Storing terraform state in azure blob or aws s3)\u003c/h3\u003e\n\u003cp\u003eWhen importing you are able to store and manage your state using blob or s3. You can do this by using the \u003ccode\u003e--backend-file\u003c/code\u003e.\nThis \u003ccode\u003e--backend-file\u003c/code\u003e will take a file path to the back end file. You can name the file \u003ccode\u003ebackend.tf\u003c/code\u003e. This backend file will use\nazure blob or aws s3 to manage the state file. To authenticate to either you will use environment variables.\u003c/p\u003e\n\u003cp\u003ePlease use \u003ccode\u003eARM_SAS_TOKEN\u003c/code\u003e or \u003ccode\u003eARM_ACCESS_KEY\u003c/code\u003e for sas token and account access key respectively for azure blob.\nPlease use \u003ccode\u003eAWS_ACCESS_KEY_ID\u003c/code\u003e and \u003ccode\u003eAWS_SECRET_ACCESS_KEY\u003c/code\u003e for the key and secret for the s3 bucket. Please go the following links in\nregards to policies and permissions. If you want to make the region dynamic you can use \u003ccode\u003eAWS_DEFAULT_REGION\u003c/code\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStoring state in \u003cstrong\u003eaws s3\u003c/strong\u003e: \u003ca href="https://www.terraform.io/docs/backends/types/s3.html"\u003ehttps://www.terraform.io/docs/backends/types/s3.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eStoring state in \u003cstrong\u003eazure blob\u003c/strong\u003e: \u003ca href="https://www.terraform.io/docs/backends/types/azurerm.html"\u003ehttps://www.terraform.io/docs/backends/types/azurerm.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id="docker-instructions"\u003eDocker instructions\u003c/h3\u003e\n\u003cdiv class="alert alert-warning"\u003e\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003e💡 \n    \u003ci class="fas fa-exclamation-triangle"\u003e\u003c/i\u003e\n\n  \n    \u003cstrong\u003ePlease be aware that the docker implementation will require you to have prior background in\nusing docker. The issues for this repository are only for reporting build failures; there wont be any direct support for docker issues.\nIf you need a stable environment to run this tool as a job please use Databricks Notebooks on a single node cluster.\u003c/strong\u003e\n  \n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003eThese set of instructions are to use docker to build and use the CLI. It avoids the need to have golang,\nTerraform, the databricks-terraform-provider to get this to run. If you do want to work on this tool please\ninstall the prior listed tools to get this to work.\u003c/p\u003e\n\u003cp\u003eTo install this tool please run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ docker build -t databricks-sync:latest .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="aliasing"\u003eAliasing\u003c/h4\u003e\n\u003cp\u003eHow our alias command works:\u003c/p\u003e\n\u003cp\u003eThis script creates 3 volume mounts with docker of which two are read only.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe mount \u003ccode\u003e$PWD\u003c/code\u003e or present working directory to \u003ccode\u003e/usr/src/databricks-sync\u003c/code\u003e as that is the working directory.\nThis allows you to manipulate files in the local working directory on your host machine.\u003c/li\u003e\n\u003cli\u003eThe second mount is mounting \u003ccode\u003e~/.databrickscfg\u003c/code\u003e to \u003ccode\u003e/root\u003c/code\u003e as that is the home directory of the container.\nThis mount is read only.\u003c/li\u003e\n\u003cli\u003eThe third mount is mounting \u003ccode\u003e~/.ssh\u003c/code\u003e folder to the \u003ccode\u003e/root/.ssh\u003c/code\u003e folder. This is so the script can fetch your\nprivate keys in a read only fashion for accessing the git repository. This is also a read only mount.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ealias dbt=\'docker run -it --rm --name docker-databricks-sync --env-file \u0026lt;(env | grep -e \u0026quot;[ARM|TF_VAR]\u0026quot;) -v \u0026quot;$PWD\u0026quot;:/usr/src/databricks-sync -v ~/.databrickscfg:/root/.databrickscfg:ro -v ~/.ssh:/root/.ssh:ro -w /usr/src/databricks-sync databricks-sync\'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="support-matrix-for-import-and-export-operations"\u003eSupport Matrix for Import and Export Operations:\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eComponent\u003c/th\u003e\n\u003cth\u003eExport\u003c/th\u003e\n\u003cth\u003eImport\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eUser Objects\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-cluster-policies/"\u003eCluster Policies\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-clusters/"\u003eClusters\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-dbfs-files/"\u003eDBFS Files\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-instance-pools/"\u003eInstance Pools\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eAWS Instance Profiles\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-jobs/"\u003eJobs\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-notebooks/"\u003eNotebooks\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-global-init-scripts/"\u003eGlobal Init Scripts\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAdministrator Setup\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAWS S3 Mounts\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAzure ADLS Gen1 Mounts\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAzure ADLS Gen2 Mounts\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAzure Blob Mount\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-secrets/"\u003eSecret\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-secrets/"\u003eSecret ACLs\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-secrets/"\u003eSecret Scopes\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMetastore Tables\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMetastore Table ACLs\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003ctd\u003e⬜️\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eUsers Management\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eGroups\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eGroup AWS Instance Profiles\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eGroup Members\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eUsers\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eUser AWS Instance Profiles\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003eAzure Service Principals\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003ctd\u003e✅\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id="project-support"\u003eProject Support\u003c/h2\u003e\n\u003cp\u003ePlease note that all projects in the /databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs).  They are provided AS-IS and we do not make any guarantees of any kind.  Please do not submit a support ticket relating to any issues arising from the use of these projects.\u003c/p\u003e\n\u003cp\u003eAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo.  They will be reviewed as time permits, but there are no formal SLAs for support.\u003c/p\u003e\n'},{id:b+1,href:"/docs/getting-started/setup/",title:"Setup",description:"Setup and install the Databricks Sync tool.",content:'\u003ch2 id="prerequisites"\u003ePrerequisites\u003c/h2\u003e\n\u003ch3 id="databricks-cli-profile-setup"\u003eDatabricks CLI Profile Setup\u003c/h3\u003e\n\u003cp\u003eDatabricks-Sync authenticates to Databricks via the Databricks CLI. The Databricks CLI should be installed and configured to authenticate to the Databricks workspace using Personal Access Tokens. For detailed instructions to install and configure the Databricks CLI, please view the official \u003ca href="https://docs.databricks.com/dev-tools/cli/index.html#databricks-cli"\u003eDatabricks documentation\u003c/a\u003e. It is a best practice to set up a unique profile within the Databricks CLI for each workspace.\u003c/p\u003e\n\u003cp\u003eCheck your Databricks CLI access credentials in the file \u003ccode\u003e~/.databrickscfg\u003c/code\u003e to verify successful authentication set up. The file should contain entries like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[profile]\n	host = https://\u0026lt;databricks-instance\u0026gt;\n	token =  \u0026lt;personal-access-token\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="git-repository"\u003eGit Repository\u003c/h3\u003e\n\u003cp\u003eEither a local repository or any repository management tool that supports SSH protocols is needed to store state. Instructions for connecting with common providers through SSH are linked below.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh"\u003eGitHub\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://support.atlassian.com/bitbucket-cloud/docs/set-up-an-ssh-key/"\u003eBitbucket Cloud\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://confluence.atlassian.com/bitbucketserver/enabling-ssh-access-to-git-repositories-in-bitbucket-server-776640358.html"\u003eBitbucket Server\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://docs.gitlab.com/ee/ssh/"\u003eGitLab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="install-terraform"\u003eInstall Terraform\u003c/h3\u003e\n\u003cp\u003eDatabricks Sync requires Terraform version \u003ca href="https://www.terraform.io/downloads.html"\u003e0.13.x or above\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe use of \u003ca href="https://github.com/tfutils/tfenv"\u003etfenv\u003c/a\u003e is encouraged to install and manage Terraform versions.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://github.com/tfutils/tfenv#installation"\u003eInstall tfenv\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003etfenv install \u0026lt;version\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTerraform version can be verified using \u003ccode\u003etfenv list\u003c/code\u003e. The active version will have an \u003ccode\u003e*\u003c/code\u003e if set. Use \u003ccode\u003etfenv use \u0026lt;version\u0026gt;\u003c/code\u003e to set the active version to one that is 0.13.x or above if necessary.\u003c/p\u003e\n\u003cp\u003eFor manual installations, refer to these instructions provided by HashiCorp Learn for \u003ca href="https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started#install-terraform"\u003eAWS\u003c/a\u003e and \u003ca href="https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/azure-get-started#install-terraform"\u003eAzure\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id="databricks-terraform-provider"\u003eDatabricks Terraform provider\u003c/h3\u003e\n\u003cp\u003eDatabricks Sync will leverage Terraform to \u003ca href="https://www.hashicorp.com/blog/automatic-installation-of-third-party-providers-with-terraform-0-13"\u003eautomatically install\u003c/a\u003e the Databricks Terraform provider when executing the \u003ccode\u003eimport\u003c/code\u003e and \u003ccode\u003eexport\u003c/code\u003e commands.\u003c/p\u003e\n\u003cp\u003eTo install manually, find the appropriate \u003ca href="https://github.com/databrickslabs/terraform-provider-databricks/releases"\u003epackage\u003c/a\u003e and download it as a zip archive. This should be \u003ca href=""\u003einstalled in the Terraform Run Environment\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="databricks-sync-installation"\u003eDatabricks Sync Installation\u003c/h2\u003e\n\u003cp\u003eExecute: \u003ccode\u003e$ pip install git+https://github.com/databrickslabs/databricks-sync.git\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003edatabricks-sync --version\u003c/code\u003e to confirm successful installation.\u003c/p\u003e\n'},{id:b+2,href:"/docs/commands/base_command/",title:"Base Command",description:"Basic outline of the different commands.",content:'\u003ch2 id="base-command"\u003eBase Command\u003c/h2\u003e\n\u003cp\u003eDatabricks-sync is used by providing a command. Each command follows the same syntax but may have distinct arguments.\u003c/p\u003e\n\u003cp\u003eA command is executed by writing the name of the command followed by any required and optional arguments. Options for the base call to databricks-sync program may be passed prior to calling the command.\u003c/p\u003e\n\u003ch3 id="command-structure"\u003eCommand structure\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003edatabricks-sync [OPTIONS] COMMAND [ARGS]...\ndatabricks-sync --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="program-options"\u003eProgram Options\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--version\u003c/code\u003e - Provides the version of databricks-sync.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--verbosity\u003c/code\u003e or \u003ccode\u003e-v\u003c/code\u003e - Either \u003ccode\u003ecritical\u003c/code\u003e, \u003ccode\u003eerror\u003c/code\u003e, \u003ccode\u003ewarning\u003c/code\u003e, \u003ccode\u003einfo\u003c/code\u003e or \u003ccode\u003edebug\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--help\u003c/code\u003e - Shows Usage, Options, and Commands for databricks-sync then exit.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="databricks-sync-commands"\u003eDatabricks Sync Commands\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href="/docs/commands/init/"\u003e\u003ccode\u003einit\u003c/code\u003e\u003c/a\u003e\u003c/strong\u003e - Create the export configuration file for running the export command.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href="/docs/commands/export/"\u003e\u003ccode\u003eexport\u003c/code\u003e\u003c/a\u003e\u003c/strong\u003e - Export objects from a source Databricks workspace.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href="/docs/commands/import/"\u003e\u003ccode\u003eimport\u003c/code\u003e\u003c/a\u003e\u003c/strong\u003e - Import objects into a target Databricks workspace.\u003c/li\u003e\n\u003c/ul\u003e\n'},{id:b+3,href:"/docs/commands/init/",title:"Init",description:"Init creates the basic structure of the export yaml file.",content:'\u003ch2 id="init"\u003eInit\u003c/h2\u003e\n\u003ch3 id="synopsis"\u003eSynopsis\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003edatabricks-sync init [OPTIONS] FILENAME\ndatabricks-sync init --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="description"\u003eDescription\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003einit\u003c/strong\u003e makes a YAML configuration file in the current working directory. This file is used for the export command.\u003c/p\u003e\n\u003ch3 id="options"\u003eOptions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--debug\u003c/code\u003e - Debug mode. Shows full stack trace on error.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--help\u003c/code\u003e or \u003ccode\u003e-h\u003c/code\u003e - Shows Usage and Options for \u003ccode\u003einit\u003c/code\u003e then exists.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="arguments"\u003eArguments\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efilename\u003c/code\u003e - A user specified name for the generated YAML configuration file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="example"\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003edatabricks-sync init EXPORT_CONFIG_FILENAME\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:b+4,href:"/docs/commands/export/",title:"Export",description:"Export uses the yaml file to export the source Databricks workspace to Git.",content:'\u003ch2 id="export"\u003eExport\u003c/h2\u003e\n\u003ch3 id="synopsis"\u003eSynopsis\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003edatabricks-sync export [--profile DATABRICKS_PROFILE_NAME default=\u0026quot;DEFAULT\u0026quot;] {-l, --local-git-path PATH | -g --git-ssh-url REPO_URL} [--branch BRANCH_NAME] -c, --config-path PATH [-k, --ssh-key-path PATH default=\u0026quot;~/.ssh/id_rsa\u0026quot;] [--dask] [--dry-run] [--debug] [--excel-report]\ndatabricks-sync export -h, --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="description"\u003eDescription\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eexport\u003c/code\u003e creates a snapshot of the Databricks-native objects within a Databricks workspace then saves this state to a provided Git repository.\u003c/p\u003e\n\u003ch3 id="options"\u003eOptions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--profile DATABRICKS_PROFILE_NAME\u003c/code\u003e - The Databricks CLI connection profile for the  source workspace. For additional information, please see the Databricks Sync \u003ca href="/docs/commands/base_command/"\u003eSetup instructions\u003c/a\u003e. If no profile was configured for the Databricks CLI during setup, then \u003ccode\u003eDEFAULT\u003c/code\u003e should be passed as the value.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-l, --local-git-path PATH\u003c/code\u003e - The path of a local git repo. Cannot be supplied in conjunction with \u003ccode\u003e-g, --git-ssh-url REPO_URL\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-g, --git-ssh-url REPO_URL\u003c/code\u003e - The URL of the remote git repo. Cannot be supplied in conjunction with \u003ccode\u003e-l | --local-git-path\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--branch BRANCH_NAME\u003c/code\u003e - This is the git repo branch of the repo designated by \u003ccode\u003e--git-ssh-url flag | --local-git-path\u003c/code\u003e. If not given, the default branch is \u003ccode\u003emaster\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-c, --config-path PATH\u003c/code\u003e - This is the relative path (to the root directory of this repo) or the full path of the yaml file which is used to drive which objects are imported/exported.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-k, --ssh-key-path PATH\u003c/code\u003e - CLI connection profile to use. The default value is \u003ccode\u003e~/.ssh/id_rsa\u003c/code\u003e. This is equivalent to the \u003ccode\u003e-i\u003c/code\u003e switch when using \u003ccode\u003essh\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--dask\u003c/code\u003e - This is a flag to use \u003ca href="https://docs.dask.org/en/latest/"\u003edask\u003c/a\u003e to parallelize the process.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--dry-run\u003c/code\u003e - This flag will log to console the actions but not commit to git remote state.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--debug\u003c/code\u003e - Debug Mode. Shows full stack trace on error.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--excel-report\u003c/code\u003e - This will export the full reporting into an excel (.xlsx) file.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-h, --help\u003c/code\u003e - Shows Usage, Options, and Arguments then exits.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="example"\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# Run export in local git\ndatabricks-sync -v debug export --profile DATABRICKS_EXPORT_PROFILE_NAME -l ~/DBFS_LOCAL_REPO_NAME -c DBFS_EXPORT_CONFIG_FILENAME.yaml --dask\n\n# Run export with remote git repo\ndatabricks-sync -v debug export --profile DATABRICKS_EXPORT_PROFILE_NAME -g REPO_URL -c DEFAULT_EXPORT_CONFIG_FILENAME.yaml --branch main --dask\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:b+5,href:"/docs/commands/import/",title:"Import",description:"Import uses the exported source files and imports to a target workspace.",content:'\u003ch2 id="import"\u003eImport\u003c/h2\u003e\n\u003ch3 id="synopsis"\u003eSynopsis\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eTF_VAR_CLOUD=\u0026quot;text\u0026quot; [TF_VAR_PASSIVE=\u0026quot;text\u0026quot;] [GIT_PYTHON_TRACE=\u0026quot;text\u0026quot;] databricks-sync import [--profile DATABRICKS_PROFILE_NAME default=\u0026quot;DEFAULT\u0026quot;] {-l, --local-git-path PATH | -g --git-ssh-url REPO_URL} [--branch BRANCH_NAME] [--revision {BRANCH | COMMIT | TAG}] [-k, --ssh-key-path] --artifact-dir PATH [--backend-file PATH] [--databricks-object-type {CLUSTER_POLICY | DBFS_FILE | NOTEBOOK | IDENTITY | INSTANCE_POOL | INSTANCE_PROFILE | SECRETS | CLUSTER | JOB}] [--plan] [--skip-refresh] [--apply] [--destroy] [--debug]\ndatabricks-sync import -h, --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="description"\u003eDescription\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eimport\u003c/code\u003e retrieves stored state from a git repository then applies this to the target Databricks workspace.\u003c/p\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eTF_VAR_CLOUD\u003c/code\u003e - Takes a value of \u003ccode\u003eaws\u003c/code\u003e, \u003ccode\u003eazure\u003c/code\u003e or \u003ccode\u003egcp\u003c/code\u003e to specify the Cloud provider. \u003cstrong\u003eRequired variable.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eTF_VAR_PASSIVE\u003c/code\u003e - Determines if databricks-sync will run in Passive Mode. The default value is set to \u003ccode\u003eFalse\u003c/code\u003e for Active-Active migrations and DR scenarios; however, it can be set \u003ccode\u003eTrue\u003c/code\u003e for Active-Passive migrations and DR scenarios.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eGIT_PYTHON_TRACE\u003c/code\u003e - Prints all the git commands run by databricks-sync. Valid value is \u0026lsquo;full\u0026rsquo;.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDATABRICKS_SYNC_IMPORT_LOCK\u003c/code\u003e - Boolean flag to lock the state file during import. This environment variable is option and default is false.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDATABRICKS_SYNC_IMPORT_PLAN_PARALLELISM\u003c/code\u003e - This is a numeric field to control the parallelism for the plan. You can set this to 1-1024.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDATABRICKS_SYNC_IMPORT_APPLY_PARALLELISM\u003c/code\u003e - This is a numeric field to control the parallelism for the apply. You can set this to 1-1024. You can use this to reduce parallelism when creating clusters and compute.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="options"\u003eOptions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--profile\u003c/code\u003e - The Databricks CLI connection profile for the  source workspace. For additional information, please see the Databricks Sync \u003ca href="https://github.com/databrickslabs/databricks-sync/blob/master/docs/setup.md"\u003eSetup instructions\u003c/a\u003e. If no profile was configured for the Databricks CLI during setup, then \u003ccode\u003eDEFAULT\u003c/code\u003e should be passed as the value.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-l, --local-git-path PATH\u003c/code\u003e - The path of a local git repo to manage export and import. Cannot be supplied in conjunction with \u003ccode\u003e-g, --git-ssh-url\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-g, --git-ssh-url REPO_URL\u003c/code\u003e - The URL of the remote git repo to manage export and import. Cannot be supplied in conjunction with \u003ccode\u003e-l | --local-git-path\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--branch\u003c/code\u003e - This is the branch of the git repo designated by \u003ccode\u003e{-g, --git-ssh-url flag | -l, --local-git-path}\u003c/code\u003e. If not given, the default branch is \u003ccode\u003emain\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--revision {BRANCH | COMMIT | TAG}\u003c/code\u003e - Specify the git repo revision which can be a branch, commit, tag.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-k, --ssh-key-path\u003c/code\u003e - CLI connection profile to use. The default value is \u003ccode\u003e~/.ssh/id_rsa\u003c/code\u003e. This is equivalent to the \u003ccode\u003e-i\u003c/code\u003e switch when using \u003ccode\u003essh\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--artifact-dir PATH\u003c/code\u003e - Path to where the plan/state file will be saved. Optional if backend state is specified through \u003ccode\u003e--backend-file\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--backend-file PATH\u003c/code\u003e - The location where backend configurations for the Terraform file will be saved.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--databricks-object-type {CLUSTER_POLICY | DBFS_FILE | NOTEBOOK | IDENTITY | INSTANCE_POOL | INSTANCE_PROFILE | SECRETS | CLUSTER | JOB}\u003c/code\u003e - This is the Databricks-native object for which to create a plan. By default, databricks-sync will plan for all objects.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--plan\u003c/code\u003e - Generates a Terraform plan for the infrastructure\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--skip-refresh\u003c/code\u003e - Determines whether the remote state will be refreshed or not\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--apply\u003c/code\u003e - Apply the plan and make modifications to the infrastructure\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--destroy\u003c/code\u003e - Indicates whether you wish to destroy all the provisioned infrastructure\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--debug\u003c/code\u003e - Debug Mode. Shows full stack trace on error.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-h, --help\u003c/code\u003e - Shows Usage, Options, and Arguments then exits.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="example"\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# Pre-apply Plan\nTF_VAR_CLOUD=azure GIT_PYTHON_TRACE=full databricks-sync -v debug import --profile DATABRICKS_PROFILE_NAME -l ~/DBFS_LOCAL_REPO_NAME --artifact-dir /dbfs/PATH --backend-file dbfs/PATH/FILES-BACKEND-CONFIG.json --plan --skip-refresh\n\n# Apply Plan\nTF_VAR_CLOUD=azure GIT_PYTHON_TRACE=full databricks-sync -v debug import --profile DATABRICKS_PROFILE_NAME -l ~/DBFS_LOCAL_REPO_NAME --artifact-dir /dbfs/PATH --backend-file dbfs/PATH/FILES-BACKEND-CONFIG.json --plan --skip-refresh --apply\n\n# Post-apply Plan\nTF_VAR_CLOUD=azure GIT_PYTHON_TRACE=full databricks-sync -v debug import --profile DATABRICKS_PROFILE_NAME -l ~/DBFS_LOCAL_REPO_NAME --artifact-dir /dbfs/PATH --backend-file dbfs/PATH/FILES-BACKEND-CONFIG.json --plan --skip-refresh\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:b+6,href:"/docs/databricks-objects/workspace-notebooks/",title:"Notebooks",description:"This guide tells you how databricks sync moves notebooks across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003cp\u003eThe impacted resources for this workspace object are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://docs.databricks.com/notebooks/index.html" target="_blank"\u003e Databricks Workspace Notebooks \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://docs.databricks.com/security/access-control/workspace-acl.html#notebook-permissions" target="_blank"\u003e Databricks Notebook Permissions \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://docs.databricks.com/security/access-control/workspace-acl.html#folder-permissions" target="_blank"\u003e Databricks Folder Permissions \u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://docs.databricks.com/dev-tools/api/latest/workspace.html" target="_blank"\u003e Workspace Api \u003c/a\u003e: Used\ndownloading and uploading notebooks via terraform provider\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://docs.databricks.com/dev-tools/api/latest/permissions.html" target="_blank"\u003e Permissions Api \u003c/a\u003e: Used\nexporting permissions on notebooks and folders and then applied to target workspace via terraform provider\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003cdiv class="alert alert-warning"\u003e\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003e💡 \n    \u003ci class="fas fa-exclamation-triangle"\u003e\u003c/i\u003e\n\n  \n    Currently there is only one limitation based on the behavior of the provider. The difference is determined by changes in\nthe source workspace. So everytime you export if it determines a difference in the checksum of the source file it will determine\nthat as a change. Any changes made or caused in the target workspace will not be tracked unless the file itself is removed.\n\n  \n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003cp\u003eAll permissions are dependant users and groups being created successfuly if you are exporting \u003ca href="/docs/databricks-objects/workspace-identities/"\u003e\u003ccode\u003eidentity\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCurrently the following permissions are exported:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePermissions directly attached to the notebooks\u003c/li\u003e\n\u003cli\u003ePermissions associated with any of the folders that are being exported. These folder permissions\nhave a dependency on at least one notebook in that folder path being created in the workspace so the folder\ncan also exist. This dependancy can be identified in the file itself\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003cp\u003eN/A. Behavior for notebooks in all clouds are the same.\u003c/p\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003cp\u003eThis section will describe the details of the export process as well as the layout.\u003c/p\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003cp\u003eHere is an example config for exporting notebooks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003ename: sample-yaml\nobjects:\n  notebook:\n    # Notebook path can be a string, a list or a YAML items collection\n    # (multiple subgroups starting with - )\n    notebook_path: \u0026quot;/\u0026quot;\n    # Use Custom map var to setup a new location.\n    # Certain patterns can be excluded from being\n    # exported via exclude_path field.\n    # Make sure to use the glob syntax to specify all paths.\n    exclude_path:\n      - \u0026quot;/Users/**\u0026quot; # Ignore all paths within the users folder\n      - \u0026quot;/tmp/**\u0026quot; # Ignore all files in the tmp directory\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003cp\u003eThere are three options that you can provide for the notebook object. They are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="#notebook-path-required"\u003e\u003ccode\u003enotebook_path\u003c/code\u003e\u003c/a\u003e \u003cstrong\u003e(required)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="#exclude-path-optional"\u003e\u003ccode\u003eexclude_path\u003c/code\u003e\u003c/a\u003e \u003cstrong\u003e(optional)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="#custom-map-vars-optional"\u003e\u003ccode\u003ecustom_map_vars\u003c/code\u003e\u003c/a\u003e \u003cstrong\u003e(optional)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id="notebook-path-required"\u003eNotebook Path (required)\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003enotebook_path\u003c/code\u003e: The path to recursively fetch all notebooks and permissions for given notebooks and folders in the provided path.\nThis field can be either a list or a string.\u003c/p\u003e\n\u003cp\u003eThe following example shows \u003ccode\u003enotebook_path\u003c/code\u003e used as a string:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003ename: sample-yaml\nobjects:\n  notebook:\n    # Notebook path can be a string, a list or a YAML items collection\n    # (multiple subgroups starting with - )\n    notebook_path: \u0026quot;/\u0026quot; # all paths inside the workspace\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following example shows \u003ccode\u003enotebook_path\u003c/code\u003e used as a list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003ename: sample-yaml\nobjects:\n  notebook:\n    # Notebook path can be a string, a list or a YAML items collection\n    # (multiple subgroups starting with - )\n    notebook_path:\n    - \u0026quot;/Shared\u0026quot; # all paths inside the /Shared location will be exported\n    - \u0026quot;/Dev\u0026quot; # all paths inside the /Dev location will be exported\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="exclude-path-optional"\u003eExclude Path (optional)\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003eexclude_path\u003c/code\u003e: The path that can be used with glob patterns (\u003ccode\u003e*\u003c/code\u003e or \u003ccode\u003e**\u003c/code\u003e) to omit the export of certain notebooks or\nsubdirectories. This field can be either a list or a string.\u003c/p\u003e\n\u003cp\u003eThe following example shows \u003ccode\u003eexclude_path\u003c/code\u003e used as a string:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003ename: sample-yaml\nobjects:\n  notebook:\n    # Notebook path can be a string, a list or a YAML items collection\n    # (multiple subgroups starting with - )\n    notebook_path: \u0026quot;/\u0026quot; # all paths inside the workspace\n    exclude_path: \u0026quot;/Users/**\u0026quot; # Ignore all paths within the users folder\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following example shows \u003ccode\u003eexclude_path\u003c/code\u003e used as a list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003ename: sample-yaml\nobjects:\n  notebook:\n    # Notebook path can be a string, a list or a YAML items collection\n    # (multiple subgroups starting with - )\n    notebook_path: \u0026quot;/\u0026quot;\n    exclude_path:\n      - \u0026quot;/Users/**\u0026quot; # Ignore all paths within the users folder\n      - \u0026quot;/tmp/**\u0026quot; # Ignore all files in the tmp directory\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="custom-map-vars-optional"\u003eCustom Map Vars (optional)\u003c/h4\u003e\n\u003cp\u003ePlease take a look at this guide to leverage custom map vars. TODO!!!\u003c/p\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003cp\u003eThe notebook resource is not dependent on other resources. The order in which resources will be created during the import is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eNotebook object\u003c/li\u003e\n\u003cli\u003eNotebook permission\u003c/li\u003e\n\u003cli\u003eParent folder permission (This will wait for atleast one notebook to be populated in that folder)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAll permissions will wait for all \u003cstrong\u003e\u003ca href="/docs/databricks-objects/workspace-identities/"\u003e\u003ccode\u003eidentity\u003c/code\u003e\u003c/a\u003e\u003c/strong\u003e to be created if they are exported.\nThis means that it will wait for all users, groups, service_principals, group members,\ninstance profiles and instance profile relationships to be created first.\u003c/p\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003cp\u003eHere is the folder lay out you should see when you run the export. Under the \u003ccode\u003eexports\u003c/code\u003e directory you should see a notebook folder\nwhich houses all of the notebook exports. You should see up to 3 types of files and one folder in the \u003ccode\u003enotebooks\u003c/code\u003e directory.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eYou should see a \u003ccode\u003edatabricks_notebook_*\u003c/code\u003e file that ends with \u003ccode\u003e.tf.json\u003c/code\u003e which contains your notebook information.\u003c/li\u003e\n\u003cli\u003eYou may see a \u003ccode\u003edatabricks_notebook_*_permissions.tf.json\u003c/code\u003e file which contains your notebook permission information.\n\u003cstrong\u003eNot every notebook will have permissions if they are inherited from the parent folders.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eYou may see a \u003ccode\u003edatabricks_folder_*_permissions.tf.json\u003c/code\u003e file which contains parent folder permission information.\n\u003cstrong\u003eOnly explicit permissions are migrated and any inherited permissions from the folder will be exported separately for that specific folder..\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the \u003ccode\u003enotebook\u003c/code\u003e directory you will also see a \u003ccode\u003edata\u003c/code\u003e folder which contains all the actual content for the notebooks.\nAll exported data will be \u003ccode\u003e.py\u003c/code\u003e, \u003ccode\u003e.scala\u003c/code\u003e, \u003ccode\u003e.sql\u003c/code\u003e or \u003ccode\u003e.r\u003c/code\u003e files.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e.\n├── databricks_spark_env.sh\n├── exports\n│    └── notebook\n│        ├── data\n│        │   ├── databricks_notebook_Shared_it_empty_python_nb.py\n│        │   ├── databricks_notebook_Shared_it_example_notebook.py\n│        │   ├── databricks_notebook_Shared_it_python_notebook.py\n│        │   ├──databricks_notebook_Shared_it_scala_notebook.scala\n│        │   └── databricks_notebook_Shared_provider_test_test123.py\n│        ├── databricks_folder_*********_permissions.tf.json\n│        ├── databricks_folder_*********_permissions.tf.json\n│        ├── databricks_notebook_*********_permissions.tf.json\n│        ├── databricks_notebook_Shared_it_empty_python_nb.tf.json\n│        ├── databricks_notebook_Shared_it_example_notebook.tf.json\n│        ├── databricks_notebook_Shared_it_python_notebook.tf.json\n│        ├── databricks_notebook_Shared_it_scala_notebook.tf.json\n│        └── databricks_notebook_Shared_provider_test_test123.tf.json\n├── terraform.tfvars\n└── variables_env.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003cp\u003eSpecific import details for importing \u003ccode\u003enotebook\u003c/code\u003e data into a target workspace.\u003c/p\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp\u003eThere are no specific environment variables that need to be specified for imports.\u003c/p\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003cp\u003eDuring the import process the increments are determined by the changes in the checksum of the \u003ccode\u003edata\u003c/code\u003e files. Any changes\ndone in the target workspace will \u003cstrong\u003enot\u003c/strong\u003e be taken into account.\u003c/p\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003cp\u003eTODO!\u003c/p\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n\u003cp\u003eTODO!\u003c/p\u003e\n'},{id:b+7,href:"/docs/databricks-objects/workspace-global-init-scripts/",title:"Global Init Scripts",description:"This guide tells you how databricks sync moves Global Init Scripts across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+8,href:"/docs/databricks-objects/workspace-cluster-policies/",title:"Cluster Policies",description:"This guide tells you how databricks sync moves Cluster Policies across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+9,href:"/docs/databricks-objects/workspace-dbfs-files/",title:"DBFS Files",description:"This guide tells you how databricks sync moves DBFS Files across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+10,href:"/docs/databricks-objects/workspace-instance-pools/",title:"Instance Pools",description:"This guide tells you how databricks sync moves Instance pools across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+11,href:"/docs/databricks-objects/workspace-secrets/",title:"Secrets",description:"This guide tells you how databricks sync moves Secrets, Scopes and ACLs across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+12,href:"/docs/databricks-objects/workspace-clusters/",title:"Clusters",description:"This guide tells you how databricks sync moves Clusters across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+13,href:"/docs/databricks-objects/workspace-jobs/",title:"Jobs",description:"This guide tells you how databricks sync moves Jobs across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+14,href:"/docs/databricks-objects/workspace-identities/",title:"Identity + Scim",description:"This guide tells you how databricks sync moves users, groups, instance profiles and relationships across workspaces.",content:'\u003ch2 id="impacted-databricks-resources"\u003eImpacted Databricks Resources\u003c/h2\u003e\n\u003ch2 id="references-to-appropriate-rest-apis"\u003eReferences to appropriate REST Apis\u003c/h2\u003e\n\u003ch2 id="known-limitations"\u003eKnown Limitations\u003c/h2\u003e\n\u003ch2 id="permissions-support"\u003ePermissions Support\u003c/h2\u003e\n\u003ch2 id="cloud-specific-behavior"\u003eCloud Specific Behavior\u003c/h2\u003e\n\u003ch2 id="export-details"\u003eExport Details\u003c/h2\u003e\n\u003ch3 id="example-config"\u003eExample Config\u003c/h3\u003e\n\u003ch3 id="config-options"\u003eConfig Options\u003c/h3\u003e\n\u003ch3 id="handled-dependencies"\u003eHandled Dependencies\u003c/h3\u003e\n\u003ch3 id="exported-content--layout"\u003eExported content + layout\u003c/h3\u003e\n\u003ch2 id="import-details"\u003eImport Details\u003c/h2\u003e\n\u003ch3 id="environment-variables"\u003eEnvironment Variables\u003c/h3\u003e\n\u003ch3 id="how-are-increments-determined"\u003eHow are increments determined\u003c/h3\u003e\n\u003ch2 id="additional-guidance"\u003eAdditional Guidance\u003c/h2\u003e\n\u003ch3 id="migrations"\u003eMigrations\u003c/h3\u003e\n\u003ch3 id="disaster-recovery"\u003eDisaster Recovery\u003c/h3\u003e\n'},{id:b+15,href:"/docs/azure-migration/export_notebook/",title:"Export Notebook",description:"This notebook is used to export all the contents for the source azure workspace.",content:'\u003ch2 id="description"\u003eDescription\u003c/h2\u003e\n\u003ch2 id="notebook-url"\u003eNotebook URL\u003c/h2\u003e\n\u003cp\u003e\u003ca href="/files/01_EXPORT.html" target="_blank"\u003e Click Here to Import Notebook \u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="notebook-details"\u003eNotebook Details\u003c/h2\u003e\n\u003cp\u003eTODO: Describe cells\u003c/p\u003e\n'},{id:b+16,href:"/docs/help/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:'\u003ch3 id="how-do-i-resolve-terraform-binary-not-being-located-when-running-export-or-import-commands"\u003eHow do I resolve Terraform binary not being located when running \u003ccode\u003eexport\u003c/code\u003e or \u003ccode\u003eimport\u003c/code\u003e commands?\u003c/h3\u003e\n\u003cp\u003eExample Error Output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e[INFO] command: terraform --version\n[ERROR] cat: /root/.tfenv/version: No such file or directory\n[ERROR] Version could not be resolved (set by /root/.tfenv/version or tfenv use \u0026lt;version\u0026gt;) Traceback (most recent call last): File \u0026quot;/usr/local/bin/databricks-sync\u0026quot;, line 8, in \u0026lt;module\u0026gt; sys.exit(cli())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eUsing \u003ccode\u003etfenv install \u0026lt;version\u0026gt;\u003c/code\u003e will automatically place the Terraform binary in the Bin directory if one version exists. If multiple Terraform versions are available, then you will need to set the desired active version using \u003ccode\u003etfenv use \u0026lt;version\u0026gt;\u003c/code\u003e, and this will update the Binaries folder.\u003c/li\u003e\n\u003cli\u003eFor Terraform binaries installed without \u003ccode\u003etfenv\u003c/code\u003e, they will need to be placed in the Bin directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n'},{id:b+17,href:"/docs/commands/",title:"Commands",description:"Commands for Databricks Sync",content:""},{id:b+18,href:"/docs/help/",title:"Help",description:"Help Doks.",content:""},{id:b+19,href:"/docs/getting-started/",title:"Getting Started",description:"Prologue Doks.",content:""},{id:b+20,href:"/docs/databricks-objects/",title:"Databricks Objects",description:"Instructions on how to sync databricks objects.",content:""},{id:b+21,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""},{id:b+22,href:"/docs/azure-migration/",title:"Migrate from Azure",description:"How to migrate from Azure Workspaces.",content:""}],f;b=b+e.length,c.add(e),f=[{id:b+0,href:"/changelog/release-0.2.0/",title:"Release 0.2.0 (Preview)",description:"Release notes for v0.2.0.",content:'\u003ch2 id="release-notes"\u003eRelease Notes\u003c/h2\u003e\n'},{id:b+1,href:"/changelog/",title:"Changelog",description:"The Doks Blog.",content:""},{id:b+2,href:"/changelog/changes/",title:"Changes",description:"Changes in Databricks Sync.",content:""}],b=b+f.length,c.add(f),userinput.addEventListener('input',g,!0),suggestions.addEventListener('click',h,!0);function g(){var g=this.value,e=c.search(g,5),f=suggestions.childNodes,h=0,i=e.length,b;for(suggestions.classList.remove('d-none'),e.forEach(function(c){b=document.createElement('div'),b.innerHTML='<a href><span></span><span></span></a>',a=b.querySelector('a'),t=b.querySelector('span:first-child'),d=b.querySelector('span:nth-child(2)'),a.href=c.href,t.textContent=c.title,d.textContent=c.description,suggestions.appendChild(b)});f.length>i;)suggestions.removeChild(f[h])}function h(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()